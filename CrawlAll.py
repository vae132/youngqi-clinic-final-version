import requests
from bs4 import BeautifulSoup
import time
import hashlib
import re
import json
import os
import datetime  # æ–°å¢ï¼Œç”¨äºè§£æå‘å¸ƒæ—¶é—´

BASE_URL = "https://andylee.pro/wp/"
# å›ºå®šé¡µé¢ï¼ˆå¦‚å…³äºé¡µé¢ï¼‰ä¸å‚ä¸ç¿»é¡µçˆ¬å–
PAGE_URLS = [
    "https://andylee.pro/wp/?page_id=11",
    "https://andylee.pro/wp/?page_id=18",
    "https://andylee.pro/wp/?page_id=1230",
    "https://andylee.pro/wp/?page_id=2115",
]
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}
TARGET_USERS = ["æå®—æ©", "andy"]

# è¿›åº¦æ–‡ä»¶ï¼Œç”¨äºè®°å½•å½“å‰é¡µç å’Œé¡µå†…æ–‡ç« åºå·ï¼ˆå‡ä»1å¼€å§‹ï¼‰
PROGRESS_FILE = "progress.txt"


def fetch_url(url, headers=HEADERS, timeout=10, max_retries=10):
    """
    å°è¯•è·å– URL å†…å®¹ï¼Œå¦‚æœå¤±è´¥åˆ™é‡è¯• max_retries æ¬¡ã€‚
    æˆåŠŸè¿”å› response å¯¹è±¡ï¼Œå¤±è´¥è¿”å› Noneã€‚
    """
    for attempt in range(1, max_retries + 1):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            if response.status_code == 200:
                return response
            else:
                print(f"âŒ å°è¯• {attempt} æ¬¡: è·å– {url} å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
        except Exception as e:
            print(f"âŒ å°è¯• {attempt} æ¬¡: è¯·æ±‚ {url} å‡ºé”™: {e}")
        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
    print(f"âŒ å·²å°è¯• {max_retries} æ¬¡ï¼Œä»æ— æ³•è·å– {url}")
    return None


def get_last_progress():
    """
    è¿”å›ä¸Šæ¬¡æˆåŠŸçˆ¬å–çš„è¿›åº¦ï¼Œæ ¼å¼ä¸º (page, order)ã€‚
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯ï¼Œåˆ™é»˜è®¤è¿”å› (1, 1)ã€‚
    """
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
            try:
                progress = json.load(f)
                page = progress.get("page", 1)
                order = progress.get("order", 1)
                return page, order
            except Exception as e:
                print(f"âŒ è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
                return 1, 1
    return 1, 1


def save_progress(page, order):
    """
    ä¿å­˜å½“å‰çˆ¬å–è¿›åº¦ï¼Œpage è¡¨ç¤ºå½“å‰é¡µç ï¼Œ
    order è¡¨ç¤ºå½“å‰é¡µä¸‹ä¸€ç¯‡éœ€è¦çˆ¬å–çš„æ–‡ç« åºå·ï¼ˆ1-indexedï¼‰ã€‚
    """
    progress = {"page": page, "order": order}
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        json.dump(progress, f)


def get_article_links(page=1):
    """
    è·å–æŒ‡å®šé¡µç çš„æ‰€æœ‰æ–‡ç« é“¾æ¥
    """
    url = f"{BASE_URL}?paged={page}"
    response = fetch_url(url)
    if not response:
        return []
    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("h2", class_="entry-title")
    links = [article.a["href"] for article in articles if article.a]
    return links


def get_article_title(article_url):
    """
    è·å–æ–‡ç« æ ‡é¢˜
    """
    response = fetch_url(article_url)
    if not response:
        return "æœªçŸ¥æ ‡é¢˜"
    soup = BeautifulSoup(response.text, "html.parser")
    title_tag = soup.find("h1", class_="post-title entry-title")
    return title_tag.text.strip() if title_tag else "æœªçŸ¥æ ‡é¢˜"


def get_article_content(article_url):
    """
    è·å–æ–‡ç« æ­£æ–‡å†…å®¹
    """
    response = fetch_url(article_url)
    if not response:
        return "æœªçŸ¥å†…å®¹"
    soup = BeautifulSoup(response.text, "html.parser")
    content_tag = soup.find("div", class_="entry-content")
    if content_tag:
        # ä½¿ç”¨ decode_contents() ä¿ç•™å†…éƒ¨ HTML æ ¼å¼
        return content_tag.decode_contents().strip()
    return "æœªçŸ¥å†…å®¹"


def get_article_time(article_url):
    """
    è·å–æ–‡ç« å‘å¸ƒæ—¶é—´ï¼Œæ ¼å¼ä¸º "YYYYå¹´MMæœˆDDæ—¥ HH:MM"
    """
    response = fetch_url(article_url)
    if not response:
        return ""
    soup = BeautifulSoup(response.text, "html.parser")
    time_span = soup.find("span", class_="entry-date post-date")
    if time_span:
        abbr_tag = time_span.find("abbr", class_="published")
        if abbr_tag and abbr_tag.has_attr("title"):
            iso_time = abbr_tag["title"]  # ä¾‹å¦‚ "2025-01-29T16:49:00-08:00"
            try:
                dt = datetime.datetime.fromisoformat(iso_time)
                formatted_time = dt.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                return formatted_time
            except Exception as e:
                print("âŒ è§£æå‘å¸ƒæ—¶é—´é”™è¯¯:", e)
                return abbr_tag.text.strip()
        else:
            return time_span.get_text(strip=True)
    return ""


def get_page_title(page_url):
    """
    è·å–å›ºå®šé¡µé¢çš„æ ‡é¢˜
    """
    response = fetch_url(page_url)
    if not response:
        return "æœªçŸ¥æ ‡é¢˜"
    soup = BeautifulSoup(response.text, "html.parser")
    title_tag = soup.find("h1")
    return title_tag.text.strip() if title_tag else "æœªçŸ¥æ ‡é¢˜"


def generate_unique_id(article_url, index):
    """
    ç”Ÿæˆå”¯ä¸€çš„IDï¼Œç»“åˆæ–‡ç« URLå’Œè¯„è®ºç´¢å¼•
    """
    return hashlib.md5(f"{article_url}-{index}".encode('utf-8')).hexdigest()


def parse_comment(comment, article_url, level=0, selected_color="white", index=0):
    """
    è§£æè¯„è®ºåŠå…¶å­è¯„è®ºï¼Œå¹¶è¿”å›æ•°æ®å­—å…¸å’Œæœ€æ–°çš„ç´¢å¼•å€¼
    """
    author_tag = comment.find("cite", class_="fn")
    if not author_tag:
        return None, index
    comment_user = author_tag.text.strip()

    time_tag = comment.find("small")
    if time_tag:
        raw_time = time_tag.get_text(strip=True)
        match = re.search(r'(\d+)\s*(\d+)\s*æœˆ,\s*(\d{4})\s+at\s+(\d+):(\d+)\s*(ä¸Šåˆ|ä¸‹åˆ)', raw_time)
        if match:
            day = int(match.group(1))
            month = int(match.group(2))
            year = int(match.group(3))
            hour = int(match.group(4))
            minute = int(match.group(5))
            period = match.group(6)
            if period == "ä¸‹åˆ" and hour < 12:
                hour += 12
            time_text = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥ {hour:02d}:{minute:02d}"
        else:
            time_text = raw_time
    else:
        time_text = ""

    comment_text_tag = comment.find("div", class_="comment_text")
    if not comment_text_tag:
        return None, index

    # åˆ é™¤è¯„è®ºä¸­çš„å›å¤æŒ‰é’®æ ‡ç­¾
    for reply in comment_text_tag.find_all("div", class_="reply"):
        reply.decompose()
    comment_text = comment_text_tag.decode_contents().strip()

    highlight = (comment_user in TARGET_USERS)
    current_index = index
    comment_id = generate_unique_id(article_url, current_index)
    index = current_index + 1

    datatest = {
        "id": comment_id,
        "author": comment_user,
        "time": time_text,
        "content": comment_text,
        "level": level,
        "highlight": highlight,
        "children": []
    }

    children_container = comment.find("ul", class_="children")
    if children_container:
        child_comments = children_container.find_all("li", class_="comment", recursive=False)
        for child in child_comments:
            child_datatest, index = parse_comment(child, article_url, level + 1, selected_color, index)
            if child_datatest:
                datatest["children"].append(child_datatest)
    return datatest, index


def get_comments(article_url, selected_color="white"):
    """
    è·å–æ–‡ç« çš„æ‰€æœ‰è¯„è®ºåŠå…¶å›å¤ï¼Œå¹¶è¿”å›è¯„è®ºæ•°æ®ï¼ˆåˆ—è¡¨å­—å…¸ï¼‰
    """
    response = fetch_url(article_url)
    if not response:
        return []
    soup = BeautifulSoup(response.text, "html.parser")
    comment_list = soup.find("ol", class_="commentlist")
    if comment_list:
        top_comments = comment_list.find_all("li", class_="comment", recursive=False)
    else:
        top_comments = soup.find_all("li", class_="comment", recursive=False)

    results = []
    index = 0
    for comment in top_comments:
        datatest, index = parse_comment(comment, article_url, selected_color=selected_color, index=index)
        if datatest:
            results.append(datatest)
    return results


def save_to_json_file(article_url, article_title, article_content, comments_datatest, page, order):
    """
    å°†çˆ¬å–çš„æ–‡ç« ä¿¡æ¯ã€æ­£æ–‡ã€å‘å¸ƒæ—¶é—´å’Œè¯„è®ºæ•°æ®ä¿å­˜ä¸º JSON æ–‡ä»¶åˆ° datatest/page{page}/ ç›®å½•ä¸‹
    """
    article_time = get_article_time(article_url)
    out = {
        "article_url": article_url,
        "title": article_title,
        "content": article_content,
        "article_time": article_time,
        "comments": comments_datatest,
        "page": page,
        "order": order
    }
    unique = generate_unique_id(article_url, order)
    folder = os.path.join("datatest", f"page{page}")
    if not os.path.exists(folder):
        os.makedirs(folder)
    filename = os.path.join(folder, f"page{page}_order{order}_{unique}.json")
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print(f"ä¿å­˜ã€Š{article_title}ã€‹è¯„è®ºæ•°æ®åˆ° {filename}")


def crawl():
    """
    çˆ¬å–è¯„è®ºå¹¶å°†æ•°æ®ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼Œæ¯é¡µæœ€å¤šå¤„ç† 10 ç¯‡æ–‡ç« ã€‚
    æ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼Œè¿›åº¦è®°å½•åŒ…å«å½“å‰é¡µå’Œé¡µå†…æ–‡ç« åºå·ã€‚
    """
    start_page, start_order = get_last_progress()
    current_page = start_page
    articles_per_page = 10

    while True:
        print(f"ğŸ“Œ æ­£åœ¨çˆ¬å–ç¬¬ {current_page} é¡µæ–‡ç« ...")
        article_links = get_article_links(current_page)
        if not article_links:
            print("ğŸš« æ²¡æœ‰æ›´å¤šæ–‡ç« ï¼Œåœæ­¢çˆ¬å–ã€‚")
            break

        # å¦‚æœå½“å‰é¡µä¸ºæ–­ç‚¹é¡µï¼Œåˆ™ä» start_order å¼€å§‹çˆ¬å–ï¼Œå¦åˆ™ä»ç¬¬ä¸€ç¯‡å¼€å§‹
        if current_page == start_page:
            article_links = article_links[start_order - 1:]
            initial_order = start_order
        else:
            initial_order = 1

        for idx, link in enumerate(article_links, start=initial_order):
            article_title = get_article_title(link)
            article_content = get_article_content(link)
            print(f"ğŸ“Œ çˆ¬å– ç¬¬ {current_page} é¡µ ç¬¬ {idx} ç¯‡: {link} | {article_title}")
            comments_datatest = get_comments(link)
            save_to_json_file(link, article_title, article_content, comments_datatest, current_page, idx)
            # æ¯æˆåŠŸå¤„ç†ä¸€ç¯‡æ–‡ç« ï¼Œæ›´æ–°è¿›åº¦è®°å½•ï¼ˆä¸‹ä¸€ç¯‡åºå·ä¸º idx+1ï¼‰
            save_progress(current_page, idx + 1)
            time.sleep(2)

        # å½“å‰é¡µå¤„ç†å®Œæˆï¼Œé‡ç½®é¡µå†…æ–‡ç« åºå·ï¼Œå¹¶è®°å½•è¿›åº¦
        start_order = 1
        save_progress(current_page, 1)
        current_page += 1
        time.sleep(3)

    # çˆ¬å–å›ºå®šé¡µé¢ï¼ˆéåˆ†é¡µé¡µé¢ï¼‰
    fixed_folder = os.path.join("datatest", "fixed")
    if not os.path.exists(fixed_folder):
        os.makedirs(fixed_folder)
    for page_url in PAGE_URLS:
        print(f"ğŸ“Œ çˆ¬å–å›ºå®šé¡µé¢: {page_url}")
        page_title = get_page_title(page_url)
        article_content = get_article_content(page_url)
        print(f"ğŸ“Œ é¡µé¢æ ‡é¢˜: {page_title}")
        comments_datatest = get_comments(page_url)
        file_id = generate_unique_id(page_url, 0)
        article_time = get_article_time(page_url)
        filename = os.path.join(fixed_folder, f"{file_id}.json")
        out = {
            "article_url": page_url,
            "title": page_title,
            "content": article_content,
            "article_time": article_time,
            "comments": comments_datatest,
            "fixed": True
        }
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)
        print(f"ä¿å­˜å›ºå®šé¡µé¢ã€Š{page_title}ã€‹åˆ° {filename}")
        time.sleep(2)
    print("\nâœ… çˆ¬å–å®Œæˆï¼Œè¯„è®ºæ•°æ®å·²ä¿å­˜åˆ° datatest ç›®å½•ä¸­ã€‚")


if __name__ == "__main__":
    crawl()
