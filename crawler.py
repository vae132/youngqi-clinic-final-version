#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import json
import hashlib
import requests
from bs4 import BeautifulSoup
import datetime  # ç”¨äºè§£æå‘å¸ƒæ—¶é—´

# =================== é…ç½®é¡¹ ===================
BASE_URL = "https://andylee.pro/wp/"
DATA_DIR = "data"       # æ•°æ®å­˜å‚¨ç›®å½•
PAGE_SIZE = 10              # æ¯é¡µä¿å­˜æ–‡ç« æ•°ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
}
TARGET_USERS = ["æå®—æ©", "andy"]  # é’ˆå¯¹ç‰¹å®šè¯„è®ºä½œè€…åšé«˜äº®å¤„ç†

# =================== åŸºç¡€çˆ¬è™«å‡½æ•° ===================

def get_article_links(page=1, retries=5):
    """
    è·å–æŒ‡å®šé¡µç çš„æ‰€æœ‰æ–‡ç« é“¾æ¥ï¼ˆæŒ‰æœ€æ–°æ’åºï¼‰
    """
    url = f"{BASE_URL}?paged={page}"
    attempt = 0
    while attempt < retries:
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            if attempt > 0:
                print(f"âœ… è·å–æ–‡ç« åˆ—è¡¨æˆåŠŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è·å–æ–‡ç« åˆ—è¡¨å‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ è·å–æ–‡ç« åˆ—è¡¨å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                return []
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    articles = soup.find_all("h2", class_="entry-title")
    links = []
    for article in articles:
        a_tag = article.find("a")
        if a_tag and "href" in a_tag.attrs:
            links.append(a_tag["href"])
    return links

def get_article_title(article_url, old_title=None, retries=5):
    """
    è·å–æ–‡ç« æ ‡é¢˜ï¼Œå…ˆæŸ¥æ‰¾ <h1 class="post-title">ï¼Œè‹¥æ— åˆ™æŸ¥æ‰¾ <h1 class="entry-title">
    è‹¥è¯·æ±‚æˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å› old_titleï¼ˆå¦‚æœæä¾›äº†ï¼‰ï¼Œå¦åˆ™è¿”å› "æœªçŸ¥æ ‡é¢˜"ã€‚
    æˆåŠŸåæ‰“å°â€œâœ… è¯·æ±‚æ–‡ç« æ ‡é¢˜æˆåŠŸâ€åŠå°è¯•æ¬¡æ•°å’Œè·å–åˆ°çš„æ ‡é¢˜ã€‚
    """
    attempt = 0
    while attempt < retries:
        try:
            response = requests.get(article_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            if attempt > 0:
                print(f"âœ… è¯·æ±‚æ–‡ç« æ ‡é¢˜æˆåŠŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è¯·æ±‚æ–‡ç« æ ‡é¢˜å‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ è¯·æ±‚æ–‡ç« æ ‡é¢˜å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                return old_title if old_title is not None else "æœªçŸ¥æ ‡é¢˜"
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    title_tag = soup.find("h1", class_="post-title")
    if not title_tag:
        title_tag = soup.find("h1", class_="entry-title")
    result = title_tag.get_text(strip=True) if title_tag else (old_title if old_title is not None else "æœªçŸ¥æ ‡é¢˜")
    print(f"âœ… è¯·æ±‚æ–‡ç« æ ‡é¢˜æˆåŠŸ, æ ‡é¢˜ä¸º: {result}")
    return result

def get_article_content(article_url, old_content=None, retries=5):
    """
    è·å–æ–‡ç« æ­£æ–‡å†…å®¹ï¼Œå°è¯•è§£æ <div class="entry-content">
    è‹¥è¯·æ±‚æˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å› old_contentï¼ˆå¦‚æœæä¾›äº†ï¼‰ï¼Œå¦åˆ™è¿”å› "æœªçŸ¥å†…å®¹"ã€‚
    æˆåŠŸåæ‰“å°â€œâœ… è¯·æ±‚æ–‡ç« å†…å®¹æˆåŠŸâ€åŠå°è¯•æ¬¡æ•°ã€‚
    """
    attempt = 0
    while attempt < retries:
        try:
            response = requests.get(article_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            if attempt > 0:
                print(f"âœ… è¯·æ±‚æ–‡ç« å†…å®¹æˆåŠŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è¯·æ±‚æ–‡ç« å†…å®¹å‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ è¯·æ±‚æ–‡ç« å†…å®¹å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                return old_content if old_content is not None else "æœªçŸ¥å†…å®¹"
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    content_tag = soup.find("div", class_="entry-content")
    result = content_tag.decode_contents().strip() if content_tag else (old_content if old_content is not None else "æœªçŸ¥å†…å®¹")
    print("âœ… è¯·æ±‚æ–‡ç« å†…å®¹æˆåŠŸ")
    return result

def get_article_time(article_url, old_time=None, retries=5):
    """
    è·å–æ–‡ç« å‘å¸ƒæ—¶é—´ï¼Œå°è¯•è§£æ <span class="entry-date post-date"> å†…çš„å‘å¸ƒæ—¶é—´ï¼Œ
    è‹¥è¯·æ±‚æˆ–è§£æå¤±è´¥ï¼Œåˆ™è¿”å› old_timeï¼ˆå¦‚æœæä¾›äº†ï¼‰ï¼Œå¦åˆ™è¿”å› ""ã€‚
    æˆåŠŸåæ‰“å°â€œâœ… è¯·æ±‚æ–‡ç« å‘å¸ƒæ—¶é—´æˆåŠŸâ€åŠå°è¯•æ¬¡æ•°ã€‚
    """
    attempt = 0
    while attempt < retries:
        try:
            response = requests.get(article_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            if attempt > 0:
                print(f"âœ… è¯·æ±‚æ–‡ç« å‘å¸ƒæ—¶é—´æˆåŠŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è¯·æ±‚æ–‡ç« å‘å¸ƒæ—¶é—´å‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ è¯·æ±‚æ–‡ç« å‘å¸ƒæ—¶é—´å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                return old_time if old_time is not None else ""
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    time_span = soup.find("span", class_="entry-date post-date")
    result = ""
    if time_span:
        abbr_tag = time_span.find("abbr", class_="published")
        if abbr_tag and abbr_tag.has_attr("title"):
            iso_time = abbr_tag["title"]
            try:
                dt = datetime.datetime.fromisoformat(iso_time)
                result = dt.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
            except Exception as e:
                print("âŒ è§£æå‘å¸ƒæ—¶é—´é”™è¯¯:", e)
                result = abbr_tag.get_text(strip=True)
        else:
            result = time_span.get_text(strip=True)
    print("âœ… è¯·æ±‚æ–‡ç« å‘å¸ƒæ—¶é—´æˆåŠŸ:", result)
    return result

def get_comments(article_url, selected_color="white", retries=5):
    """
    è·å–æ–‡ç« çš„æ‰€æœ‰è¯„è®ºåŠå…¶å›å¤ï¼Œå¹¶è¿”å›è¯„è®ºæ•°æ®ï¼ˆåˆ—è¡¨å­—å…¸ï¼‰ã€‚
    å¦‚æœè¯·æ±‚æˆåŠŸä½†é¡µé¢ä¸­æ— è¯„è®ºï¼ˆä¾‹å¦‚æ–‡ç« æœ¬èº«æ²¡æœ‰è¯„è®ºï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨å¹¶æ‰“å°ç›¸åº”æç¤ºï¼›
    å¦‚æœè¯·æ±‚å§‹ç»ˆå¤±è´¥ï¼Œåˆ™è¿”å› None å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯ã€‚
    """
    attempt = 0
    response = None
    while attempt < retries:
        try:
            response = requests.get(article_url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            if attempt > 0:
                print(f"âœ… è¯·æ±‚æ–‡ç« è¯„è®ºæˆåŠŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è¯·æ±‚æ–‡ç« è¯„è®ºå‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ è¯·æ±‚è¯„è®ºå¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
                return None
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    comment_list = soup.find("ol", class_="commentlist")
    if comment_list:
        top_comments = comment_list.find_all("li", class_="comment", recursive=False)
    else:
        top_comments = soup.find_all("li", class_="comment", recursive=False)
    results = []
    index = 0
    for comment in top_comments:
        data, index = parse_comment(comment, article_url, selected_color=selected_color, index=index)
        if data:
            results.append(data)
    if response is not None:
        if len(results) == 0:
            print("âœ… è¯·æ±‚æ–‡ç« è¯„è®ºæˆåŠŸï¼Œä½†æ–‡ç« æœ¬èº«æ²¡æœ‰è¯„è®º")
        else:
            print(f"âœ… è¯·æ±‚æ–‡ç« è¯„è®ºæˆåŠŸ, å…±è·å– {len(results)} æ¡è¯„è®º")
    return results

# ------------------- ä»¥ä¸‹ä¸ºè¯„è®ºè§£æç›¸å…³ -------------------

def generate_unique_id(article_url, index):
    """
    ç”Ÿæˆå”¯ä¸€çš„IDï¼Œç»“åˆæ–‡ç« URLå’Œè¯„è®ºç´¢å¼•
    """
    return hashlib.md5(f"{article_url}-{index}".encode('utf-8')).hexdigest()

def parse_comment(comment, article_url, level=0, selected_color="white", index=0):
    """
    è§£æè¯„è®ºåŠå…¶å­è¯„è®ºï¼Œå¹¶è¿”å›æ•°æ®å­—å…¸å’Œæœ€æ–°çš„ç´¢å¼•å€¼
    """
    author_tag = comment.find("cite", class_="fn")
    if not author_tag:
        return None, index
    comment_user = author_tag.text.strip()

    time_tag = comment.find("small")
    if time_tag:
        raw_time = time_tag.get_text(strip=True)
        match = re.search(r'(\d+)\s*(\d+)\s*æœˆ,\s*(\d{4})\s+at\s+(\d+):(\d+)\s*(ä¸Šåˆ|ä¸‹åˆ)', raw_time)
        if match:
            day = int(match.group(1))
            month = int(match.group(2))
            year = int(match.group(3))
            hour = int(match.group(4))
            minute = int(match.group(5))
            period = match.group(6)
            if period == "ä¸‹åˆ" and hour < 12:
                hour += 12
            time_text = f"{year}å¹´{month:02d}æœˆ{day:02d}æ—¥ {hour:02d}:{minute:02d}"
        else:
            time_text = raw_time
    else:
        time_text = ""

    comment_text_tag = comment.find("div", class_="comment_text")
    if not comment_text_tag:
        return None, index

    # åˆ é™¤è¯„è®ºä¸­çš„å›å¤æŒ‰é’®æ ‡ç­¾
    for reply in comment_text_tag.find_all("div", class_="reply"):
        reply.decompose()
    comment_text = comment_text_tag.decode_contents().strip()

    highlight = (comment_user in TARGET_USERS)
    current_index = index
    comment_id = generate_unique_id(article_url, current_index)
    index = current_index + 1

    data = {
        "id": comment_id,
        "author": comment_user,
        "time": time_text,
        "content": comment_text,
        "level": level,
        "highlight": highlight,
        "children": []
    }

    children_container = comment.find("ul", class_="children")
    if children_container:
        child_comments = children_container.find_all("li", class_="comment", recursive=False)
        for child in child_comments:
            child_data, index = parse_comment(child, article_url, level + 1, selected_color, index)
            if child_data:
                data["children"].append(child_data)
    return data, index

# ------------------- ä»¥ä¸‹ä¸ºæ•°æ®å­˜å‚¨ä¸æ›´æ–°é€»è¾‘ -------------------

def save_to_json_file(article_data, page, order, fixed=False):
    """
    å°† article_data ä¿å­˜ä¸º JSON æ–‡ä»¶åˆ°ç›¸åº”ç›®å½•ä¸­
    å¦‚æœ fixed ä¸º Falseï¼Œåˆ™ä¿å­˜åˆ° data/page{page} ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åæ ¼å¼ï¼špage{page}_order{order}_{unique}.json
    å¦‚æœ fixed ä¸º Trueï¼Œåˆ™ä¿å­˜åˆ° data/fixed ç›®å½•ä¸‹ï¼Œæ–‡ä»¶åä¿æŒåŸæ–‡ä»¶åï¼ˆè‹¥å­˜åœ¨ï¼‰æˆ–æ–°ç”Ÿæˆ
    """
    unique = generate_unique_id(article_data["article_url"], order)
    if not fixed:
        folder = os.path.join(DATA_DIR, f"page{page}")
        if not os.path.exists(folder):
            os.makedirs(folder)
        filename = os.path.join(folder, f"page{page}_order{order}_{unique}.json")
    else:
        folder = os.path.join(DATA_DIR, "fixed")
        if not os.path.exists(folder):
            os.makedirs(folder)
        filename = article_data.get("filename")
        if not filename:
            filename = os.path.join(folder, f"{unique}.json")
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(article_data, f, ensure_ascii=False, indent=2)
    return filename

def load_all_local_articles():
    """
    éå† DATA_DIR ä¸‹æ‰€æœ‰ page æ–‡ä»¶å¤¹ï¼ŒåŠ è½½æ‰€æœ‰ JSON æ–‡ä»¶ï¼Œ
    è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼ŒåŒ…å« article_url, title, content, article_time, comments, page, order, filename ç­‰å­—æ®µã€‚
    åˆ—è¡¨æŒ‰é¡µç å’Œ order å‡åºæ’åºï¼ˆpage1_order1 ä¸ºæœ€æ–°æ–‡ç« ï¼‰
    """
    articles = []
    if not os.path.exists(DATA_DIR):
        return articles
    for folder in os.listdir(DATA_DIR):
        folder_path = os.path.join(DATA_DIR, folder)
        if os.path.isdir(folder_path) and folder.startswith("page"):
            m = re.search(r'page(\d+)', folder)
            if not m:
                continue
            page_num = int(m.group(1))
            for filename in os.listdir(folder_path):
                if filename.endswith(".json"):
                    filepath = os.path.join(folder_path, filename)
                    try:
                        with open(filepath, "r", encoding="utf-8") as f:
                            data = json.load(f)
                        m2 = re.search(r'order(\d+)', filename)
                        order_num = int(m2.group(1)) if m2 else 0
                        data["page"] = page_num
                        data["order"] = order_num
                        data["filename"] = filepath
                        articles.append(data)
                    except Exception as e:
                        print(f"âŒ åŠ è½½æ–‡ä»¶ {filepath} å‡ºé”™: {e}")
    articles.sort(key=lambda x: (x["page"], x["order"]))
    return articles

def load_fixed_articles():
    """
    éå† DATA_DIR/fixed ç›®å½•ï¼ŒåŠ è½½æ‰€æœ‰ JSON æ–‡ä»¶ï¼Œ
    è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼ŒåŒ…å« article_url, title, content, article_time, comments, filename ç­‰å­—æ®µã€‚
    """
    articles = []
    fixed_dir = os.path.join(DATA_DIR, "fixed")
    if not os.path.exists(fixed_dir):
        return articles
    for filename in os.listdir(fixed_dir):
        if filename.endswith(".json"):
            filepath = os.path.join(fixed_dir, filename)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    data = json.load(f)
                data["filename"] = filepath
                articles.append(data)
            except Exception as e:
                print(f"âŒ åŠ è½½å›ºå®šé¡µé¢æ–‡ä»¶ {filepath} å‡ºé”™: {e}")
    return articles

def reassign_and_save_articles(all_articles):
    """
    å°†æ‰€æœ‰æ–‡ç« æŒ‰ç…§é¡ºåºé‡æ–°åˆ†é…é¡µç å’Œ orderï¼Œ
    æ¸…ç©ºåŸ DATA_DIR ä¸‹çš„ JSON æ–‡ä»¶åï¼Œä¾ PAGE_SIZE åˆ†é¡µå†™å…¥æ–‡ä»¶ã€‚
    """
    new_articles = all_articles[:]  # æ‹·è´åˆ—è¡¨
    # æ¸…ç©º DATA_DIR ä¸‹æ‰€æœ‰ JSON æ–‡ä»¶ï¼ˆå»ºè®®äº‹å…ˆå¤‡ä»½ï¼‰
    if os.path.exists(DATA_DIR):
        for root, dirs, files in os.walk(DATA_DIR):
            for f in files:
                # å¦‚æœæ˜¯ fixed ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ™è·³è¿‡åˆ é™¤
                if f.endswith(".json") and "fixed" not in root:
                    os.remove(os.path.join(root, f))
    else:
        os.makedirs(DATA_DIR)
    for idx, article in enumerate(new_articles):
        page = idx // PAGE_SIZE + 1
        order = idx % PAGE_SIZE + 1
        article["page"] = page
        article["order"] = order
        save_to_json_file(article, page, order)
    print("âœ… é‡æ–°åˆ†é…å¹¶ä¿å­˜æ–‡ç« å®Œæˆï¼")

# =================== æ–°æ–‡ç« æ›´æ–°ç›¸å…³ ===================

def get_current_website_articles(max_pages=1):
    """
    ä»ç½‘ç«™å‰ max_pages é¡µä¸­è·å–æ–‡ç« é“¾æ¥åˆ—è¡¨ï¼Œé»˜è®¤åªå–ç¬¬ä¸€é¡µï¼ˆæ–°æ–‡ç« æ€»åœ¨æœ€å‰é¢ï¼‰
    """
    links = []
    for page in range(1, max_pages + 1):
        page_links = get_article_links(page)
        if not page_links:
            break
        links.extend(page_links)
    return links

def fetch_new_articles(new_urls):
    """
    é’ˆå¯¹æ¯ä¸ªæ–°çš„æ–‡ç«  URLï¼Œçˆ¬å–æ ‡é¢˜ã€æ­£æ–‡ã€å‘å¸ƒæ—¶é—´å’Œè¯„è®ºï¼Œè¿”å›æ–‡ç« æ•°æ®åˆ—è¡¨
    """
    new_articles = []
    for url in new_urls:
        print(f"çˆ¬å–æ–°æ–‡ç« ï¼š{url}")
        title = get_article_title(url)
        content = get_article_content(url)
        article_time = get_article_time(url)
        comments = get_comments(url)
        article_data = {
            "article_url": url,
            "title": title,
            "content": content,
            "article_time": article_time,
            "comments": comments,  # å¦‚æœè¯·æ±‚æˆåŠŸä½†æ— è¯„è®ºï¼Œåˆ™ comments ä¸º []ï¼ˆæœ‰æ•ˆç»“æœï¼‰
            "timestamp": time.time()
        }
        new_articles.append(article_data)
        time.sleep(2)
    return new_articles

def update_new_articles():
    """
    æ£€æŸ¥ç½‘ç«™æœ€æ–°æ–‡ç« ä¸æœ¬åœ° data/page ç¬¬ä¸€ç¯‡æ˜¯å¦ä¸€è‡´ï¼Œ
    è‹¥æœ‰æ–°æ–‡ç« åˆ™æ–°æ–‡ç« å§‹ç»ˆæ’å…¥åœ¨æœ€å‰é¢ï¼ŒåŸæ–‡ç« åç§»ï¼Œ
    ä¸”åªæœ‰å½“ n ç¯‡æ–°æ–‡ç« å…¨éƒ¨éƒ½æˆåŠŸçˆ¬å–åˆ°æœ‰æ•ˆæ ‡é¢˜ã€æ­£æ–‡ã€å‘å¸ƒæ—¶é—´å’Œè¯„è®º
    ï¼ˆå³æ ‡é¢˜ä¸ä¸º â€œæœªçŸ¥æ ‡é¢˜â€ï¼Œå†…å®¹ä¸ä¸º â€œæœªçŸ¥å†…å®¹â€ï¼Œå‘å¸ƒæ—¶é—´ä¸ä¸ºç©ºï¼Œä¸”è¯„è®ºæ•°æ®ä¸ä¸º Noneï¼›æ³¨æ„ï¼šå¦‚æœæ–‡ç« æœ¬èº«æ— è¯„è®ºï¼Œè¿”å› [] æ˜¯æœ‰æ•ˆç»“æœï¼‰æ—¶ï¼Œ
    æ‰å°† n ç¯‡æ–°æ–‡ç« åˆå¹¶åŸæœ‰æ–‡ç« åé‡æ–°åˆ†é…é¡µç å’Œé¡ºåºå†™å…¥æ–‡ä»¶ã€‚
    """
    print("æ£€æŸ¥ç½‘ç«™æœ€æ–°æ–‡ç« æ˜¯å¦æœ‰æ›´æ–°â€¦â€¦")
    # å¯¹è·å–æœ€æ–°æ–‡ç« é“¾æ¥ç»™äºˆæœ€å¤š 5 æ¬¡æœºä¼š
    attempt = 0
    website_links = []
    while attempt < 5:
        website_links = get_current_website_articles(max_pages=1)
        if website_links:
            print(f"âœ… æˆåŠŸè·å–ç½‘ç«™æœ€æ–°æ–‡ç« é“¾æ¥ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        attempt += 1
        time.sleep(5)
    if not website_links:
        print("âŒ 5 æ¬¡å°è¯•åä»æ— æ³•è·å–ç½‘ç«™æœ€æ–°æ–‡ç« é“¾æ¥")
        return

    local_articles = load_all_local_articles()
    first_local_url = local_articles[0]["article_url"] if local_articles else None

    new_count = 0
    for link in website_links:
        if link == first_local_url:
            break
        new_count += 1

    if new_count == 0:
        print("âœ… æœ¬åœ°æ•°æ®å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°æ–‡ç« ã€‚")
        return
    else:
        print(f"âœ… æ£€æµ‹åˆ° {new_count} ç¯‡æ–°æ–‡ç« ã€‚")
        new_urls = website_links[0:new_count]

    # å¯¹ n ç¯‡æ–°æ–‡ç« çš„çˆ¬å–ï¼Œç»™äºˆæœ€å¤š 5 æ¬¡æœºä¼šï¼Œè¦æ±‚å…¨éƒ¨æ–‡ç« éƒ½çˆ¬å–æˆåŠŸ
    attempt = 0
    all_valid = False
    new_articles = []
    while attempt < 5:
        new_articles = fetch_new_articles(new_urls)
        # ä»…å½“ get_comments è¿”å› None æ‰è§†ä¸ºè¯·æ±‚å¤±è´¥ï¼›è‹¥è¿”å› [] åˆ™è®¤ä¸ºæ–‡ç« æœ¬èº«æ— è¯„è®ºï¼Œæ˜¯æœ‰æ•ˆç»“æœ
        invalid_articles = [article for article in new_articles if article["title"] == "æœªçŸ¥æ ‡é¢˜"
                            or article["content"] == "æœªçŸ¥å†…å®¹"
                            or not article["article_time"]
                            or article["comments"] is None]
        if not invalid_articles:
            all_valid = True
            break
        else:
            print(f"ç¬¬ {attempt+1} æ¬¡å°è¯•çˆ¬å–æ–°æ–‡ç« æœªæˆåŠŸï¼Œé—®é¢˜æ–‡ç« : {', '.join([article['article_url'] for article in invalid_articles])}")
            attempt += 1
            time.sleep(5)
    if not all_valid:
        print("âŒ 5 æ¬¡å°è¯•åä»æœ‰æ–‡ç« çˆ¬å–ä¸æˆåŠŸï¼Œæ–°æ–‡ç« ä¸å†™å…¥æ–‡ä»¶")
        return

    # å¦‚æœé‡æ–°çˆ¬å–åå…¨éƒ¨æˆåŠŸï¼Œåˆ™æ‰“å°æˆåŠŸæ ‡å¿—
    print("âœ… æ–°æ–‡ç« å…¨éƒ¨çˆ¬å–æˆåŠŸï¼")

    # å…¨éƒ¨ n ç¯‡æ–°æ–‡ç« å‡çˆ¬å–æˆåŠŸï¼Œåˆå¹¶æ–°æ–‡ç« å’Œæ—§æ–‡ç« ï¼Œå¹¶é‡æ–°åˆ†é…é¡µç åå†™å…¥æ–‡ä»¶
    merged_articles = new_articles + local_articles
    reassign_and_save_articles(merged_articles)

# =================== è¿‘æœŸç•™è¨€æ›´æ–°ï¼ˆæŒ‰æ–‡ç« æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´åŒ¹é…ï¼‰ ===================

def get_recent_comment_articles_collection(retries=5):
    """
    ç›´æ¥çˆ¬å–æ•´ä¸ªè¿‘æœŸè¯„è®ºåŒºåŸŸï¼Œæå–æ¯æ¡è¯„è®ºä¸­æ¶‰åŠçš„æ–‡ç« æ ‡é¢˜å’Œé“¾æ¥ï¼Œ
    å¹¶æ„é€ ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºæ–‡ç« æ ‡é¢˜ï¼Œå€¼ä¸ºå¯¹åº”çš„æ–‡ç« é“¾æ¥ã€‚
    å‡è®¾è¿‘æœŸè¯„è®ºåŒºåŸŸåœ¨ <aside id="recent-comments-5"> å†…ï¼Œ
    æ¯ä¸ªè¯„è®ºé¡¹åœ¨ <li class="recentcomments"> ä¸­ï¼Œ
    ä¸”æ–‡ç« é“¾æ¥åœ¨è¯¥ li ä¸­çš„ç¬¬äºŒä¸ª <a> æ ‡ç­¾å†…ï¼ˆå¦‚æœå­˜åœ¨å¤šä¸ª <a> æ ‡ç­¾ï¼Œå¦åˆ™ä¸ºç¬¬ä¸€ä¸ªï¼‰ã€‚
    ç»™äºˆæœ€å¤š retries æ¬¡æœºä¼š
    """
    url = BASE_URL  # ä»¥é¦–é¡µä¸ºä¾‹
    attempt = 0
    while attempt < retries:
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            print(f"âœ… æˆåŠŸè·å–è¿‘æœŸè¯„è®ºåŒºåŸŸ (å°è¯•ç¬¬ {attempt+1} æ¬¡)")
            break
        except Exception as e:
            attempt += 1
            print(f"âŒ è·å–è¿‘æœŸè¯„è®ºåŒºåŸŸå‡ºé”™ï¼š{e}, å°è¯•ç¬¬ {attempt} æ¬¡")
            if attempt == retries:
                print("âŒ 5 æ¬¡å°è¯•åä»æ— æ³•è·å–è¿‘æœŸè¯„è®ºåŒºåŸŸ")
                return {}
            time.sleep(2)
    soup = BeautifulSoup(response.text, "html.parser")
    recent_comments = soup.find("aside", id="recent-comments-5")
    if not recent_comments:
        print("âœ… æœªæ‰¾åˆ°è¿‘æœŸè¯„è®ºåŒºåŸŸ")
        return {}
    title_to_link = {}
    comment_items = recent_comments.find_all("li", class_="recentcomments")
    for li in comment_items:
        a_tags = li.find_all("a", href=True)
        # å¦‚æœæœ‰ä¸¤ä¸ªæˆ–æ›´å¤šé“¾æ¥ï¼Œå–ç¬¬äºŒä¸ªä¸ºæ–‡ç« æ ‡é¢˜é“¾æ¥ï¼›å¦åˆ™å–ç¬¬ä¸€ä¸ª
        if len(a_tags) >= 2:
            a_tag = a_tags[1]
        elif a_tags:
            a_tag = a_tags[0]
        else:
            continue
        title = a_tag.get_text(strip=True)
        link = a_tag["href"]
        if title not in title_to_link:
            title_to_link[title] = link
    return title_to_link

def update_recent_comments_by_title():
    """
    å¯¹äºè¿‘æœŸç•™è¨€ä¸­æ¶‰åŠçš„æ–‡ç« ï¼Œ
    å…ˆçˆ¬å–æ•´ä¸ªè¿‘æœŸè¯„è®ºåŒºåŸŸå¾—åˆ°ã€æ ‡é¢˜, é“¾æ¥ã€‘é›†åˆï¼Œ
    ç„¶ååœ¨æœ¬åœ°æ•°æ®ä¸­æ ¹æ®æ ‡é¢˜å’Œæ–‡ç« å‘å¸ƒæ—¶é—´æŸ¥æ‰¾å¯¹åº”æ–‡ç« ï¼ˆå…ˆåœ¨ data/page ä¸­æŸ¥æ‰¾ï¼Œè‹¥æ‰¾ä¸åˆ°å†åœ¨ data/fixed ä¸­æŸ¥æ‰¾ï¼‰ï¼Œ
    å¦‚æœæ‰¾åˆ°åˆ™é‡æ–°çˆ¬å–è¯¥æ–‡ç« çš„æ•°æ®ï¼ˆåŒ…æ‹¬æ ‡é¢˜ã€æ­£æ–‡ã€å‘å¸ƒæ—¶é—´å’Œè¯„è®ºï¼‰ï¼Œ
    åªæœ‰å½“çˆ¬å–åˆ°çš„æ•°æ®æœ‰æ•ˆæ—¶æ‰æ›´æ–°ï¼Œå¦åˆ™ä¿ç•™åŸæ•°æ®ã€‚
    å¦‚æœçˆ¬å–åˆ°çš„æ–‡ç« å‘å¸ƒæ—¶é—´ä¸ºç©ºï¼Œåˆ™é€€å›åˆ°ç”¨æ–‡ç«  URL è¿›è¡ŒåŒ¹é…ã€‚
    """
    print("å¼€å§‹æ£€æŸ¥è¿‘æœŸç•™è¨€æ›´æ–°ï¼ˆæŒ‰æ–‡ç« æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´åŒ¹é…ï¼‰â€¦â€¦")
    title_to_url = get_recent_comment_articles_collection()
    if not title_to_url:
        print("è¿‘æœŸç•™è¨€æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ç« æ•°æ®ã€‚")
        return

    local_articles = load_all_local_articles()  # data/page ä¸‹çš„æ–‡ç« 
    fixed_articles = load_fixed_articles()        # data/fixed ä¸‹çš„æ–‡ç« 
    updated = 0
    for title, url in title_to_url.items():
        # å…ˆè·å–ç½‘é¡µä¸Šæœ€æ–°çš„å‘å¸ƒæ—¶é—´ï¼Œç”¨äºåŒ¹é…
        new_article_time = get_article_time(url, old_time="")
        match_found = None
        location = ""
        # å¦‚æœçˆ¬å–åˆ°å‘å¸ƒæ—¶é—´ï¼Œåˆ™åŒæ—¶åŒ¹é…æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´
        if new_article_time:
            for article in local_articles:
                if article["title"] == title and article.get("article_time", "") == new_article_time:
                    match_found = article
                    location = "å¸¸è§„é¡µé¢"
                    break
            if not match_found:
                for article in fixed_articles:
                    if article["title"] == title and article.get("article_time", "") == new_article_time:
                        match_found = article
                        location = "å›ºå®šé¡µé¢"
                        break
        # å¦‚æœå‘å¸ƒæ—¶é—´ä¸ºç©ºæˆ–åŒ¹é…å¤±è´¥ï¼Œåˆ™é€€å›åˆ°ä½¿ç”¨ URL è¿›è¡ŒåŒ¹é…
        if not match_found:
            for article in local_articles:
                if article.get("article_url", "") == url:
                    match_found = article
                    location = "å¸¸è§„é¡µé¢"
                    break
            if not match_found:
                for article in fixed_articles:
                    if article.get("article_url", "") == url:
                        match_found = article
                        location = "å›ºå®šé¡µé¢"
                        break
        if match_found:
            if location == "å¸¸è§„é¡µé¢":
                print(f"ğŸ“Œ æ­£åœ¨çˆ¬å–ç¬¬ {match_found.get('page', '?')} é¡µ ç¬¬ {match_found.get('order', '?')} ç¯‡æ–‡ç« ï¼š{title}")
            else:
                print(f"ğŸ“Œ æ­£åœ¨çˆ¬å–å›ºå®šé¡µé¢ï¼š{title}")
            new_title = get_article_title(url, old_title=match_found["title"])
            if new_title != "æœªçŸ¥æ ‡é¢˜":
                match_found["title"] = new_title
            else:
                print(f"âŒ æ ‡é¢˜çˆ¬å–å¤±è´¥ï¼Œä¿ç•™åŸæœ‰æ ‡é¢˜ï¼š{match_found['title']}")
            new_content = get_article_content(url, old_content=match_found.get("content"))
            if new_content != "æœªçŸ¥å†…å®¹":
                match_found["content"] = new_content
            else:
                print(f"âŒ æ­£æ–‡çˆ¬å–å¤±è´¥ï¼Œä¿ç•™åŸæœ‰å†…å®¹")
            new_time = get_article_time(url, old_time=match_found.get("article_time"))
            if new_time:
                match_found["article_time"] = new_time
            else:
                print(f"âŒ å‘å¸ƒæ—¶é—´çˆ¬å–å¤±è´¥ï¼Œä¿ç•™åŸæœ‰å‘å¸ƒæ—¶é—´")
            new_comments = get_comments(url)
            if new_comments is None:
                print(f"âŒ è¯„è®ºçˆ¬å–å¤±è´¥ï¼š{title}ï¼Œä¿ç•™åŸæœ‰è¯„è®º")
            else:
                match_found["comments"] = new_comments
                match_found["timestamp"] = time.time()
            try:
                with open(match_found["filename"], "w", encoding="utf-8") as f:
                    json.dump(match_found, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ›´æ–°å®Œæˆï¼š{location} - {match_found['title']}")
                updated += 1
            except Exception as e:
                print(f"âŒ ä¿å­˜æ›´æ–°å¤±è´¥ï¼ˆæ ‡é¢˜ï¼š{match_found['title']}ï¼‰ï¼š{e}")
            time.sleep(2)
        else:
            print(f"âŒ æœªåœ¨æœ¬åœ°æ•°æ®ä¸­æ‰¾åˆ°åŒ¹é…æ–‡ç« ï¼ˆæ ‡é¢˜åŠå‘å¸ƒæ—¶é—´ä¸åŒ¹é…ï¼‰ï¼š{title}")
    print(f"âœ… è¿‘æœŸç•™è¨€æŒ‰æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´åŒ¹é…æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {updated} ç¯‡æ–‡ç« ã€‚")

# =================== ä¸»æ›´æ–°æµç¨‹ ===================

def main_update():
    """
    ä¸»æµç¨‹ï¼š
    1. æ£€æŸ¥ç½‘ç«™æ˜¯å¦æœ‰æ–°æ–‡ç« ï¼Œå¦‚æœ‰åˆ™æ›´æ–°æ–‡ç« å¹¶é‡æ–°åˆ†é…é¡µç ä¸é¡ºåºï¼›
    2. æ£€æŸ¥è¿‘æœŸç•™è¨€ä¸­æ¶‰åŠçš„æ–‡ç« ï¼ŒæŒ‰æ–‡ç« æ ‡é¢˜å’Œå‘å¸ƒæ—¶é—´åŒ¹é…æ›´æ–°å…¶æ•°æ®ï¼›
    3. æ‰“å°æ›´æ–°å®Œæˆæç¤ºã€‚
    """
    update_new_articles()
    update_recent_comments_by_title()
    print("âœ… æ‰€æœ‰æ›´æ–°å®Œæˆï¼")

if __name__ == "__main__":
    main_update()
